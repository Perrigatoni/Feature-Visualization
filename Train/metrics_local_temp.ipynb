{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a 10x10 confusion matrix the values for True Positive, True Negative and False Positive, False Negative are a bit more confusingly placed than those regarding a simpler binary classification task. \n",
    "</br>\n",
    "**True_Positive** : Is the singular value regarding a class label. For example, for class 6, the True Positive value is the cell at indices [5,5].\n",
    "</br>\n",
    "**True_Negative** : Is everything else on the confusion matrix that isn't on the line and column of the True Positive Value. In our specific example, everything except row 5 and column 5 is part of True Negative and has to be sum'ed.\n",
    "</br>\n",
    "**False_Positive** : Everything on the previously ignored column (with the exception of the correct value for TP) is called False Positive.\n",
    "</br>\n",
    "**False_Negative** : Everything on the previously ignored row  (with the exception of the value for TP) is called False Negative."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As it might have become obvious by now, we have to calculate all these scores for all of the 10 classes in our dataset. That means each class will in the end have their own PRecision, Recall and F1-score.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[666   8  50  33  70  54  16  44  43  16]\n",
      " [ 10 774   3   6   3  34  90  71   9   0]\n",
      " [112  18 479  60 157  47  13  21  90   3]\n",
      " [ 32  15  55 462 203 159  14  51   8   1]\n",
      " [ 65   8 139 137 514  64  10  23  37   3]\n",
      " [ 46  56  32 121  62 485  27 148  23   0]\n",
      " [ 21 130  10   4   5  17 762  34  17   0]\n",
      " [ 40 111  18  54  13 169  33 527  30   5]\n",
      " [ 57   9  91  10  18  23  19  13 757   3]\n",
      " [ 28   1   2   0   1   1   2   1   3 961]]\n",
      "Mean Accuracy: 63.870000000000005 %\n",
      "Class 0: Precision:0.6183844011142061, Recall:0.666, F1-Score:0.6413095811266251\n",
      "Class 1: Precision:0.6849557522123894, Recall:0.774, F1-Score:0.7267605633802818\n",
      "Class 2: Precision:0.5449374288964732, Recall:0.479, F1-Score:0.5098456625864822\n",
      "Class 3: Precision:0.5208568207440811, Recall:0.462, F1-Score:0.4896661367249603\n",
      "Class 4: Precision:0.491395793499044, Recall:0.514, F1-Score:0.5024437927663734\n",
      "Class 5: Precision:0.46058879392212726, Recall:0.485, F1-Score:0.47247929858743304\n",
      "Class 6: Precision:0.7728194726166329, Recall:0.762, F1-Score:0.7673716012084593\n",
      "Class 7: Precision:0.564844587352626, Recall:0.527, F1-Score:0.545266425245732\n",
      "Class 8: Precision:0.744346116027532, Recall:0.757, F1-Score:0.7506197322756569\n",
      "Class 9: Precision:0.96875, Recall:0.961, F1-Score:0.9648594377510041\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_parquet(path=r'C:\\Users\\Noel\\Documents\\THESIS\\Feature Visualization\\dataframe_test65')\n",
    "# print(df)\n",
    "\n",
    "conf_mat = df.to_numpy()\n",
    "print(conf_mat)\n",
    "Mean_Acc = 0\n",
    "for i in range(len(conf_mat)):\n",
    "    Mean_Acc += conf_mat[i, i] / 1000\n",
    "print(f\"Mean Accuracy: {(Mean_Acc * 10):.4f} %\")\n",
    "class_score_dict = {}\n",
    "for class_label in range(len(conf_mat)):\n",
    "\n",
    "    TP = conf_mat[class_label, class_label]\n",
    "    temp_conf = np.copy(conf_mat)\n",
    "    temp_conf[class_label, class_label] = 0\n",
    "    FP = sum(temp_conf[:, class_label])\n",
    "    FN = sum(temp_conf[class_label, :])\n",
    "\n",
    "    temp_conf = np.copy(conf_mat)\n",
    "    temp_conf[:, class_label] = 0\n",
    "    temp_conf[class_label, :] = 0\n",
    "\n",
    "    # TN = sum(sum(temp_conf))\n",
    "    TN = np.mean(temp_conf)\n",
    "    \n",
    "\n",
    "    # Calculate scores and save them to dictionary\n",
    "    Precision = TP / (TP + FP)\n",
    "    Recall = TP / (TP + FN)\n",
    "    F1 = (Precision * Recall / (Precision + Recall)) * 2 \n",
    "\n",
    "    class_score_dict[class_label] = (Precision, Recall, F1)\n",
    "\n",
    "for key, value in class_score_dict.items():\n",
    "    print(f'Class {key}: Precision:{value[0]:.4f}, Recall:{value[1]:.4f}, F1-Score:{value[2]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchCUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
