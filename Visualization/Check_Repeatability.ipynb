{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling of suspected duplicate images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r'C:\\Users\\Noel\\Documents\\THESIS\\Feature Visualization\\Dataframes\\reapeatability_resnet34.parquet')\n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers = df.columns.tolist()\n",
    "# layers = layers[3:]\n",
    "# layer_dict = {}\n",
    "# for index in range(len(layers)):\n",
    "#     layer_dict[index] = layers[index]\n",
    "# print(layer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv1', 'layer1.0.conv1', 'layer1.0.conv2', 'layer1.1.conv1', 'layer1.1.conv2', 'layer1.2.conv1', 'layer1.2.conv2', 'layer2.0.conv1', 'layer2.0.conv2', 'layer2.0.downsample.0', 'layer2.1.conv1', 'layer2.1.conv2', 'layer2.2.conv1', 'layer2.2.conv2', 'layer2.3.conv1', 'layer2.3.conv2', 'layer3.0.conv1', 'layer3.0.conv2', 'layer3.0.downsample.0', 'layer3.1.conv1', 'layer3.1.conv2', 'layer3.2.conv1', 'layer3.2.conv2', 'layer3.3.conv1', 'layer3.3.conv2', 'layer3.4.conv1', 'layer3.4.conv2', 'layer3.5.conv1', 'layer3.5.conv2', 'layer4.0.conv1', 'layer4.0.conv2', 'layer4.0.downsample.0', 'layer4.1.conv1', 'layer4.1.conv2', 'layer4.2.conv1', 'layer4.2.conv2', 'fc']\n",
      "{0: 'conv1', 1: 'layer1.0.conv1', 2: 'layer1.0.conv2', 3: 'layer1.1.conv1', 4: 'layer1.1.conv2', 5: 'layer1.2.conv1', 6: 'layer1.2.conv2', 7: 'layer2.0.conv1', 8: 'layer2.0.conv2', 9: 'layer2.0.downsample.0', 10: 'layer2.1.conv1', 11: 'layer2.1.conv2', 12: 'layer2.2.conv1', 13: 'layer2.2.conv2', 14: 'layer2.3.conv1', 15: 'layer2.3.conv2', 16: 'layer3.0.conv1', 17: 'layer3.0.conv2', 18: 'layer3.0.downsample.0', 19: 'layer3.1.conv1', 20: 'layer3.1.conv2', 21: 'layer3.2.conv1', 22: 'layer3.2.conv2', 23: 'layer3.3.conv1', 24: 'layer3.3.conv2', 25: 'layer3.4.conv1', 26: 'layer3.4.conv2', 27: 'layer3.5.conv1', 28: 'layer3.5.conv2', 29: 'layer4.0.conv1', 30: 'layer4.0.conv2', 31: 'layer4.0.downsample.0', 32: 'layer4.1.conv1', 33: 'layer4.1.conv2', 34: 'layer4.2.conv1', 35: 'layer4.2.conv2', 36: 'fc'}\n"
     ]
    }
   ],
   "source": [
    "# type(df['conv1'][0])\n",
    "layers_of_interest = [name for name in df.columns.tolist() if \"conv\" in name or \"fc\" in name or \"downsample\" in name]\n",
    "print(layers_of_interest)\n",
    "layer_dict = {}\n",
    "for index in range(len(layers_of_interest)):\n",
    "    layer_dict[index] = layers_of_interest[index]\n",
    "print(layer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['layer'] = df['layer'].map(lambda x: layer_dict[x])\n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column with which to sort paths\n",
    "layer = 'layer3.2.conv1'\n",
    "channel = 78\n",
    "assert channel <= len(df[layer]), f\"Channel Index out of range. {layer} has {len(df[layer])} channels.\"\n",
    "column_name = f'activations_{layer}_channel_{channel}'\n",
    "df[column_name] = df[layer].map(lambda x:x[channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=column_name, inplace=True, ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The immediate following commented out since it cannot work the same way... will delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_labels =  df['class_label'].head(20).tolist()\n",
    "# top_predictions =  df['prediction'].head(20).tolist()\n",
    "# type(top_labels)\n",
    "# corrects = 0\n",
    "# for i in range(len(top_labels)):\n",
    "#     if top_predictions[i] == top_labels[i]:\n",
    "#         corrects += 1\n",
    "# percentage = corrects / len(top_labels) * 100\n",
    "# print(f'Percentage of class labels exciting the channel and being of correct prediction: {percentage}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe try the zip method, or enumerate to make the tuple, although this one seems simple and straightforward enough, it does not inform of any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top iamges that activate maximally to pass to grid function\n",
    "amount = 20\n",
    "paths = df[\"path\"][:amount].tolist()\n",
    "layer_names = df[\"layer\"][:amount].tolist()\n",
    "\n",
    "# Make tuple of (image_path, layer_name)\n",
    "top_images = [(paths[i], layer_names[i]) for i in range(len(paths))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Specify font... maybe do arial.ttf if nothing else works\n",
    "font = ImageFont.truetype(\"MOD20.TTF\", 35)\n",
    "position = (15, 180)\n",
    "\n",
    "images_to_disp = []\n",
    "# you need a font to change the size of the letters\n",
    "for image_path, layer_name in list(top_images):\n",
    "    img_w_legend = Image.open(image_path)\n",
    "    legend = ImageDraw.Draw(img_w_legend)\n",
    "    # Define bounding box to place image legend in\n",
    "    bbox = legend.textbbox(position, layer_name, font=font)\n",
    "    # bbox = font.getbbox(layer_name)  # newer method, but the syntax should be different... maybe investigate?\n",
    "    # Design rectangular shape with bbox coordinates\n",
    "    legend.rectangle(xy=bbox, fill='white')\n",
    "    # Place the respective text in legend\n",
    "    legend.text(position, layer_name, font=font, fill='black')\n",
    "    # Convert to tensor so you can use the 'make_grid' function\n",
    "    make_tensor = ToTensor()\n",
    "    # Append to list\n",
    "    images_to_disp.append(make_tensor(img_w_legend))\n",
    "\n",
    "# Make the grid\n",
    "grid = make_grid(images_to_disp, nrow=5, padding=0)\n",
    "\n",
    "# Pass the grid as PIL Image to display or save\n",
    "img = torchvision.transforms.ToPILImage()(grid)\n",
    "img.show()\n",
    "# img.save(fp=r\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchCUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18ff6e8b2fd13bf6d8a6c622bd97b42372209098e930592ee4b22e7c3c3c40a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
